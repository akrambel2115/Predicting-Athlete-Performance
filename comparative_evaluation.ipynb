{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34aad1ea",
   "metadata": {},
   "source": [
    "# Comparative Evaluation of Search Algorithms for Athlete Training Schedule Optimization\n",
    "\n",
    "This notebook compares three different search algorithms applied to the athlete training schedule optimization problem:\n",
    "1. **Constraint Satisfaction Problem (CSP)** with backtracking search\n",
    "2. **Depth-First Search (DFS)**\n",
    "3. **Breadth-First Search (BFS)**\n",
    "4. **Uniform Cost Search (UCS)**\n",
    "5. **A-star Search**\n",
    "6. **Greedy Best-First Search**\n",
    "7. **Genetic Algorithm**\n",
    "\n",
    "We will compare these algorithms across several metrics:\n",
    "- **Time Complexity**: Execution time\n",
    "- **Space Complexity**: Memory usage and maximum frontier size\n",
    "- **Solution Quality**: Performance level, fatigue management, and injury risk\n",
    "- **Training Schedule Properties**: Rest/training balance, workout intensity distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591d3d6",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tracemalloc\n",
    "\n",
    "# Import our search algorithm implementations\n",
    "from csp import AthleteTrainingCSP\n",
    "from dfs_search import DFSSearch\n",
    "from BFS_search import BFSSearch\n",
    "from UCS_Search import UCSSearch\n",
    "from A_star import AStarSearch\n",
    "from greedy_search_implementation import GreedySearch\n",
    "from Problem import AthletePerformanceProblem\n",
    "from Genetic import GeneticAlgorithm as SimpleGeneticAlgorithm\n",
    "from Genetic_akram import GeneticAlgorithm as AkramGeneticAlgorithm\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f95dd",
   "metadata": {},
   "source": [
    "## Define Common Parameters\n",
    "\n",
    "To ensure a fair comparison, we'll use the same initial conditions and constraints for all algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for all search algorithms\n",
    "INITIAL_STATE = (0, 1.5, 0.2, 5.5)  # day, fatigue, risk, performance\n",
    "TARGET_DAY = 10                      # 10-day schedule\n",
    "TARGET_PERF = 6.8                      # Target performance level\n",
    "MAX_FATIGUE = 3.0                    # Maximum allowable fatigue\n",
    "MAX_RISK = 0.35                       # Maximum allowable injury risk\n",
    "\n",
    "# Time limit for CSP algorithm (in seconds)\n",
    "CSP_TIME_LIMIT = 120\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6018244",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's create some helper functions to standardize the evaluation process and results display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf21b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(func):\n",
    "    \"\"\"Decorator to measure execution time and memory usage of a function\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Start memory tracking\n",
    "        tracemalloc.start()\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Execute function\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Get memory usage\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        \n",
    "        # Return results along with measurements\n",
    "        return {\n",
    "            'result': result,\n",
    "            'execution_time': execution_time,\n",
    "            'peak_memory': peak / 1024 / 1024  # Convert to MB\n",
    "        }\n",
    "    return wrapper\n",
    "\n",
    "def evaluate_schedule(problem, actions, algorithm_name):\n",
    "    \"\"\"Evaluates a schedule and extracts metrics\"\"\"\n",
    "    if not actions:\n",
    "        print(f\"No solution found for {algorithm_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize from the initial state\n",
    "    state = problem.initial_state\n",
    "    state_progression = [state]\n",
    "    \n",
    "    # Apply each action and track the state progression\n",
    "    for action in actions:\n",
    "        state = problem.apply_action(state, action)\n",
    "        state_progression.append(state)\n",
    "        \n",
    "    # Extract metrics from the final state\n",
    "    day, fatigue, risk, performance, _ = state_progression[-1]\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    rest_days = sum(1 for action in actions if action[0] == 0.0)\n",
    "    high_intensity_days = sum(1 for action in actions if action[0] >= 0.7)\n",
    "    total_workload = sum(problem.LOAD_PER_MIN.get(action[0], 0) * action[1] for action in actions)\n",
    "    \n",
    "    # Compute state progression for plotting\n",
    "    performance_progression = [s[3] for s in state_progression]\n",
    "    fatigue_progression = [s[1] for s in state_progression]\n",
    "    risk_progression = [s[2] for s in state_progression]\n",
    "    \n",
    "    return {\n",
    "        'algorithm': algorithm_name,\n",
    "        'final_day': day,\n",
    "        'final_performance': performance,\n",
    "        'final_fatigue': fatigue,\n",
    "        'final_risk': risk,\n",
    "        'goal_achieved': day >= TARGET_DAY and performance >= TARGET_PERF,\n",
    "        'rest_days': rest_days,\n",
    "        'high_intensity_days': high_intensity_days,\n",
    "        'total_workload': total_workload,\n",
    "        'actions': actions,\n",
    "        'performance_progression': performance_progression,\n",
    "        'fatigue_progression': fatigue_progression,\n",
    "        'risk_progression': risk_progression\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1e2ca",
   "metadata": {},
   "source": [
    "## Run CSP Algorithm\n",
    "\n",
    "First, we'll run the Constraint Satisfaction Problem (CSP) approach with backtracking search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c90bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_performance\n",
    "def run_csp():\n",
    "    problem = AthleteTrainingCSP(\n",
    "        initial_state=INITIAL_STATE,\n",
    "        target_day=TARGET_DAY,\n",
    "        max_fatigue=MAX_FATIGUE,\n",
    "        max_risk=MAX_RISK\n",
    "    )\n",
    "    \n",
    "    solution = problem.backtracking_search(time_limit=CSP_TIME_LIMIT)\n",
    "    \n",
    "    # Return both solution and stats\n",
    "    return {\n",
    "        'solution': solution,\n",
    "        'stats': problem.backtrack_stats\n",
    "    }\n",
    "\n",
    "# Run the CSP algorithm and measure its performance\n",
    "print(\"Running CSP algorithm...\")\n",
    "csp_result = run_csp()\n",
    "csp_solution = csp_result['result']['solution']\n",
    "csp_stats = csp_result['result']['stats']\n",
    "\n",
    "# Create temporary problem to evaluate final state\n",
    "if csp_solution:\n",
    "    temp_problem = AthletePerformanceProblem(initial_state=INITIAL_STATE, target_day=TARGET_DAY)\n",
    "    current_state = temp_problem.initial_state\n",
    "    for action in csp_solution:\n",
    "        current_state = temp_problem.apply_action(current_state, action)\n",
    "    day, fatigue, risk, performance, _ = current_state\n",
    "    best_solution_msg = f\"Performance: {performance:.2f}, Risk: {risk:.3f}, Fatigue: {fatigue:.2f}\"\n",
    "else:\n",
    "    best_solution_msg = \"No solution found\"\n",
    "\n",
    "print(f\"CSP execution time: {csp_result['execution_time']:.2f} seconds\")\n",
    "print(f\"CSP peak memory usage: {csp_result['peak_memory']:.2f} MB\")\n",
    "print(f\"CSP backtracking iterations: {csp_stats['iterations']}\")\n",
    "print(f\"CSP maximum depth reached: {csp_stats['max_depth']}\")\n",
    "print(f\"Best solution found: {best_solution_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e63d95",
   "metadata": {},
   "source": [
    "## Run DFS Algorithm\n",
    "\n",
    "Now we'll run the Depth-First Search (DFS) algorithm with a reduced target day of 4 for uninformed search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a38053",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_performance\n",
    "def run_dfs():\n",
    "    problem = AthletePerformanceProblem(\n",
    "        initial_state=INITIAL_STATE,\n",
    "        target_day=4  # Reduced target for uninformed search\n",
    "    )\n",
    "    \n",
    "    searcher = DFSSearch(problem)\n",
    "    searcher.problem.target_day = 4  # Reduced target for uninformed search\n",
    "    searcher.problem.target_perf = TARGET_PERF\n",
    "    searcher.problem.max_fatigue = MAX_FATIGUE\n",
    "    searcher.problem.max_risk = MAX_RISK\n",
    "    \n",
    "    goal_node = searcher.search()\n",
    "    \n",
    "    if goal_node is None:\n",
    "        print(\"DFS found no solution\")\n",
    "        path = []\n",
    "    else:\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "    \n",
    "    # Return both solution and stats\n",
    "    return {\n",
    "        'path': path,\n",
    "        'expanded_nodes': searcher.expanded_nodes,\n",
    "        'max_stack_size': searcher.max_stack_size\n",
    "    }\n",
    "\n",
    "# Run the DFS algorithm and measure its performance\n",
    "print(\"Running DFS algorithm...\")\n",
    "dfs_result = run_dfs()\n",
    "dfs_path = dfs_result['result']['path']\n",
    "dfs_expanded_nodes = dfs_result['result']['expanded_nodes']\n",
    "dfs_max_stack_size = dfs_result['result']['max_stack_size']\n",
    "\n",
    "# Create temporary problem to evaluate final state\n",
    "if dfs_path:\n",
    "    temp_problem = AthletePerformanceProblem(initial_state=INITIAL_STATE, target_day=4)\n",
    "    current_state = temp_problem.initial_state\n",
    "    for action in dfs_path:\n",
    "        current_state = temp_problem.apply_action(current_state, action)\n",
    "    day, fatigue, risk, performance, _ = current_state\n",
    "    best_solution_msg = f\"Performance: {performance:.2f}, Risk: {risk:.3f}, Fatigue: {fatigue:.2f}\"\n",
    "else:\n",
    "    best_solution_msg = \"No solution found\"\n",
    "\n",
    "print(f\"DFS execution time: {dfs_result['execution_time']:.2f} seconds\")\n",
    "print(f\"DFS peak memory usage: {dfs_result['peak_memory']:.2f} MB\")\n",
    "print(f\"DFS nodes expanded: {dfs_expanded_nodes}\")\n",
    "print(f\"DFS maximum stack size: {dfs_max_stack_size}\")\n",
    "print(f\"Best solution found: {best_solution_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c1d57b",
   "metadata": {},
   "source": [
    "## Run BFS Algorithm\n",
    "\n",
    "Next, we'll run the Breadth-First Search (BFS) algorithm with a reduced target day of 4 for uninformed search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3018038",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_performance\n",
    "def run_bfs():\n",
    "    problem = AthletePerformanceProblem(\n",
    "        initial_state=INITIAL_STATE,\n",
    "        target_day=4  # Reduced target for uninformed search\n",
    "    )\n",
    "    \n",
    "    searcher = BFSSearch(problem)\n",
    "    searcher.problem.target_day = 4  # Reduced target for uninformed search\n",
    "    searcher.problem.target_perf = TARGET_PERF\n",
    "    searcher.problem.max_fatigue = MAX_FATIGUE\n",
    "    searcher.problem.max_risk = MAX_RISK\n",
    "    \n",
    "    goal_node = searcher.search()\n",
    "    \n",
    "    if goal_node is None:\n",
    "        print(\"BFS found no solution\")\n",
    "        path = []\n",
    "    else:\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "    \n",
    "    # Return both solution and stats\n",
    "    return {\n",
    "        'path': path,\n",
    "        'expanded_nodes': searcher.expanded_nodes,\n",
    "        'max_queue_size': searcher.max_queue_size\n",
    "    }\n",
    "\n",
    "# Run the BFS algorithm and measure its performance\n",
    "print(\"Running BFS algorithm...\")\n",
    "bfs_result = run_bfs()\n",
    "bfs_path = bfs_result['result']['path']\n",
    "bfs_expanded_nodes = bfs_result['result']['expanded_nodes']\n",
    "bfs_max_queue_size = bfs_result['result']['max_queue_size']\n",
    "\n",
    "# Create temporary problem to evaluate final state\n",
    "if bfs_path:\n",
    "    temp_problem = AthletePerformanceProblem(initial_state=INITIAL_STATE, target_day=4)\n",
    "    current_state = temp_problem.initial_state\n",
    "    for action in bfs_path:\n",
    "        current_state = temp_problem.apply_action(current_state, action)\n",
    "    day, fatigue, risk, performance, _ = current_state\n",
    "    best_solution_msg = f\"Performance: {performance:.2f}, Risk: {risk:.3f}, Fatigue: {fatigue:.2f}\"\n",
    "else:\n",
    "    best_solution_msg = \"No solution found\"\n",
    "\n",
    "print(f\"BFS execution time: {bfs_result['execution_time']:.2f} seconds\")\n",
    "print(f\"BFS peak memory usage: {bfs_result['peak_memory']:.2f} MB\")\n",
    "print(f\"BFS nodes expanded: {bfs_expanded_nodes}\")\n",
    "print(f\"BFS maximum queue size: {bfs_max_queue_size}\")\n",
    "print(f\"Best solution found: {best_solution_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57430994",
   "metadata": {},
   "source": [
    "## Run UCS Algorithm\n",
    "\n",
    "Now we'll run the Uniform Cost Search (UCS) which expands nodes based on path cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_performance\n",
    "def run_ucs():\n",
    "    problem = AthletePerformanceProblem(\n",
    "        initial_state=INITIAL_STATE,\n",
    "        target_day=TARGET_DAY\n",
    "    )\n",
    "    \n",
    "    searcher = UCSSearch(problem)\n",
    "    searcher.problem.target_day = TARGET_DAY\n",
    "    searcher.problem.target_perf = TARGET_PERF\n",
    "    searcher.problem.max_fatigue = MAX_FATIGUE\n",
    "    searcher.problem.max_risk = MAX_RISK\n",
    "    \n",
    "    goal_node = searcher.search()\n",
    "    \n",
    "    if goal_node is None:\n",
    "        print(\"UCS found no solution\")\n",
    "        path = []\n",
    "    else:\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "    \n",
    "    # Return both solution and stats\n",
    "    return {\n",
    "        'path': path,\n",
    "        'expanded_nodes': searcher.expanded_nodes,\n",
    "        'max_queue_size': searcher.max_queue_size\n",
    "    }\n",
    "\n",
    "# Run the UCS algorithm and measure its performance\n",
    "print(\"Running UCS algorithm...\")\n",
    "ucs_result = run_ucs()\n",
    "ucs_path = ucs_result['result']['path']\n",
    "ucs_expanded_nodes = ucs_result['result']['expanded_nodes']\n",
    "ucs_max_queue_size = ucs_result['result']['max_queue_size']\n",
    "\n",
    "# Create temporary problem to evaluate final state\n",
    "if ucs_path:\n",
    "    temp_problem = AthletePerformanceProblem(initial_state=INITIAL_STATE, target_day=TARGET_DAY)\n",
    "    current_state = temp_problem.initial_state\n",
    "    for action in ucs_path:\n",
    "        current_state = temp_problem.apply_action(current_state, action)\n",
    "    day, fatigue, risk, performance, _ = current_state\n",
    "    best_solution_msg = f\"Performance: {performance:.2f}, Risk: {risk:.3f}, Fatigue: {fatigue:.2f}\"\n",
    "else:\n",
    "    best_solution_msg = \"No solution found\"\n",
    "\n",
    "print(f\"UCS execution time: {ucs_result['execution_time']:.2f} seconds\")\n",
    "print(f\"UCS peak memory usage: {ucs_result['peak_memory']:.2f} MB\")\n",
    "print(f\"UCS nodes expanded: {ucs_expanded_nodes}\")\n",
    "print(f\"UCS maximum queue size: {ucs_max_queue_size}\")\n",
    "print(f\"Best solution found: {best_solution_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94d7db",
   "metadata": {},
   "source": [
    "## Run A* Search Algorithm\n",
    "\n",
    "Now we'll run the A* Search which uses a combination of path cost and heuristic to find an optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_performance\n",
    "def run_astar():\n",
    "    problem = AthletePerformanceProblem(\n",
    "        initial_state=INITIAL_STATE,\n",
    "        target_day=TARGET_DAY\n",
    "    )\n",
    "    \n",
    "    searcher = AStarSearch(problem)\n",
    "    searcher.problem.target_day = TARGET_DAY\n",
    "    searcher.problem.target_perf = TARGET_PERF\n",
    "    searcher.problem.max_fatigue = MAX_FATIGUE\n",
    "    searcher.problem.max_risk = MAX_RISK\n",
    "    \n",
    "    goal_node = searcher.search()\n",
    "    \n",
    "    if goal_node is None:\n",
    "        print(\"A* search found no solution\")\n",
    "        path = []\n",
    "    else:\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "    \n",
    "    # Return both solution and stats\n",
    "    return {\n",
    "        'path': path,\n",
    "        'expanded_nodes': searcher.expanded_nodes,\n",
    "        'max_queue_size': searcher.max_queue_size\n",
    "    }\n",
    "\n",
    "# Run the A* Search algorithm and measure its performance\n",
    "print(\"Running A* Search algorithm...\")\n",
    "astar_result = run_astar()\n",
    "astar_path = astar_result['result']['path']\n",
    "astar_expanded_nodes = astar_result['result']['expanded_nodes']\n",
    "astar_max_queue_size = astar_result['result']['max_queue_size']\n",
    "\n",
    "# Create temporary problem to evaluate final state\n",
    "if astar_path:\n",
    "    temp_problem = AthletePerformanceProblem(initial_state=INITIAL_STATE, target_day=TARGET_DAY)\n",
    "    current_state = temp_problem.initial_state\n",
    "    for action in astar_path:\n",
    "        current_state = temp_problem.apply_action(current_state, action)\n",
    "    day, fatigue, risk, performance, _ = current_state\n",
    "    best_solution_msg = f\"Performance: {performance:.2f}, Risk: {risk:.3f}, Fatigue: {fatigue:.2f}\"\n",
    "else:\n",
    "    best_solution_msg = \"No solution found\"\n",
    "\n",
    "print(f\"A* Search execution time: {astar_result['execution_time']:.2f} seconds\")\n",
    "print(f\"A* Search peak memory usage: {astar_result['peak_memory']:.2f} MB\")\n",
    "print(f\"A* Search nodes expanded: {astar_expanded_nodes}\")\n",
    "print(f\"A* Search maximum queue size: {astar_max_queue_size}\")\n",
    "print(f\"Best solution found: {best_solution_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa49e3",
   "metadata": {},
   "source": [
    "## Run Greedy Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_performance\n",
    "def run_greedy():\n",
    "    problem = AthletePerformanceProblem(\n",
    "        initial_state=INITIAL_STATE,\n",
    "        target_day=TARGET_DAY\n",
    "    )\n",
    "    \n",
    "    searcher = GreedySearch(problem)\n",
    "    searcher.problem.target_day = TARGET_DAY\n",
    "    searcher.problem.target_perf = TARGET_PERF\n",
    "    searcher.problem.max_fatigue = MAX_FATIGUE\n",
    "    searcher.problem.max_risk = MAX_RISK\n",
    "    \n",
    "    goal_node = searcher.search()\n",
    "    \n",
    "    if goal_node is None:\n",
    "        print(\"Greedy search found no solution\")\n",
    "        path = []\n",
    "    else:\n",
    "        path = searcher.reconstruct_path(goal_node)\n",
    "    \n",
    "    # Return both solution and stats\n",
    "    return {\n",
    "        'path': path,\n",
    "        'expanded_nodes': searcher.expanded_nodes,\n",
    "        'max_queue_size': searcher.max_queue_size\n",
    "    }\n",
    "\n",
    "# Run the Greedy Search algorithm and measure its performance\n",
    "print(\"Running Greedy Search algorithm...\")\n",
    "greedy_result = run_greedy()\n",
    "greedy_path = greedy_result['result']['path']\n",
    "greedy_expanded_nodes = greedy_result['result']['expanded_nodes']\n",
    "greedy_max_queue_size = greedy_result['result']['max_queue_size']\n",
    "\n",
    "# Create temporary problem to evaluate final state\n",
    "if greedy_path:\n",
    "    temp_problem = AthletePerformanceProblem(initial_state=INITIAL_STATE, target_day=TARGET_DAY)\n",
    "    current_state = temp_problem.initial_state\n",
    "    for action in greedy_path:\n",
    "        current_state = temp_problem.apply_action(current_state, action)\n",
    "    day, fatigue, risk, performance, _ = current_state\n",
    "    best_solution_msg = f\"Performance: {performance:.2f}, Risk: {risk:.3f}, Fatigue: {fatigue:.2f}\"\n",
    "else:\n",
    "    best_solution_msg = \"No solution found\"\n",
    "\n",
    "print(f\"Greedy execution time: {greedy_result['execution_time']:.2f} seconds\")\n",
    "print(f\"Greedy peak memory usage: {greedy_result['peak_memory']:.2f} MB\")\n",
    "print(f\"Greedy nodes expanded: {greedy_expanded_nodes}\")\n",
    "print(f\"Greedy maximum queue size: {greedy_max_queue_size}\")\n",
    "print(f\"Best solution found: {best_solution_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3dde97",
   "metadata": {},
   "source": [
    "## Run Genetic Algorithm\n",
    "\n",
    "Now, we'll run the Genetic Algorithm from Genetic_akram.py to optimize the athlete training schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @measure_performance\n",
    "# def run_genetic():\n",
    "#     # Create athlete performance problem instance with genetic flag\n",
    "#     problem = AthletePerformanceProblem(\n",
    "#         initial_state=INITIAL_STATE,\n",
    "#         genetic=True  # Flag to use genetic-specific implementation\n",
    "#     )\n",
    "    \n",
    "#     # Set target parameters for the problem\n",
    "#     problem.target_day = TARGET_DAY\n",
    "#     problem.target_perf = TARGET_PERF\n",
    "#     problem.max_fatigue = MAX_FATIGUE\n",
    "#     problem.max_risk = MAX_RISK\n",
    "    \n",
    "#     # Create the genetic algorithm instance\n",
    "#     ga = AkramGeneticAlgorithm(\n",
    "#         problem=problem,\n",
    "#         population_size=100,  # Size of the population\n",
    "#         num_generations=30,   # Number of generations to run\n",
    "#         mutation_rate=0.05    # Mutation rate\n",
    "#     )\n",
    "    \n",
    "#     # Run the genetic algorithm\n",
    "#     best_schedule, best_fitness = ga.run()\n",
    "    \n",
    "#     return {\n",
    "#         'schedule': best_schedule,\n",
    "#         'fitness': best_fitness,\n",
    "#         'ga_instance': ga\n",
    "#     }\n",
    "\n",
    "# # Run the Genetic Algorithm and measure its performance\n",
    "# print(\"Running Genetic Algorithm...\")\n",
    "# genetic_result = run_genetic()\n",
    "# genetic_schedule = genetic_result['result']['schedule']\n",
    "# genetic_fitness = genetic_result['result']['fitness']\n",
    "# genetic_ga_instance = genetic_result['result']['ga_instance']\n",
    "\n",
    "# # Extract fitness metrics (assuming they're returned in the order performance, risk, fatigue)\n",
    "# performance, risk, fatigue = genetic_fitness\n",
    "\n",
    "# print(f\"Genetic execution time: {genetic_result['execution_time']:.2f} seconds\")\n",
    "# print(f\"Genetic peak memory usage: {genetic_result['peak_memory']:.2f} MB\")\n",
    "# print(f\"Genetic Algorithm cache size: {len(genetic_ga_instance.state_cache) if hasattr(genetic_ga_instance, 'state_cache') else 'N/A'} states evaluated\")\n",
    "# print(f\"Best solution found: Performance: {performance:.2f}, Risk: {risk:.3f}, Fatigue: {fatigue:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c6951",
   "metadata": {},
   "source": [
    "## Evaluate Solution Quality\n",
    "\n",
    "Now that we have solutions from all three algorithms, let's evaluate their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a common problem instance for evaluation\n",
    "eval_problem = AthletePerformanceProblem(\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_day=TARGET_DAY\n",
    ")\n",
    "# Set additional attributes after creation\n",
    "eval_problem.target_perf = TARGET_PERF\n",
    "eval_problem.max_fatigue = MAX_FATIGUE\n",
    "eval_problem.max_risk = MAX_RISK\n",
    "\n",
    "# Evaluate each solution\n",
    "csp_evaluation = evaluate_schedule(eval_problem, csp_solution, \"CSP\")\n",
    "dfs_evaluation = evaluate_schedule(eval_problem, dfs_path, \"DFS\")\n",
    "bfs_evaluation = evaluate_schedule(eval_problem, bfs_path, \"BFS\")\n",
    "ucs_evaluation = evaluate_schedule(eval_problem, ucs_path, \"UCS\")\n",
    "greedy_evaluation = evaluate_schedule(eval_problem, greedy_path, \"Greedy\")\n",
    "astar_evaluation = evaluate_schedule(eval_problem, astar_path, \"A*\")\n",
    "# genetic_evaluation = evaluate_schedule(eval_problem, genetic_schedule, \"Genetic\")\n",
    "\n",
    "# Combine all evaluations into a list for comparison\n",
    "evaluations = []\n",
    "if csp_evaluation:\n",
    "    evaluations.append(csp_evaluation)\n",
    "if dfs_evaluation:\n",
    "    evaluations.append(dfs_evaluation)\n",
    "if bfs_evaluation:\n",
    "    evaluations.append(bfs_evaluation)\n",
    "if ucs_evaluation:\n",
    "    evaluations.append(ucs_evaluation)\n",
    "if greedy_evaluation:\n",
    "    evaluations.append(greedy_evaluation)\n",
    "if astar_evaluation:\n",
    "    evaluations.append(astar_evaluation)\n",
    "# if genetic_evaluation:\n",
    "#     evaluations.append(genetic_evaluation)\n",
    "\n",
    "# Create a DataFrame for easy comparison\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Algorithm': eval['algorithm'],\n",
    "        'Final Performance': round(eval['final_performance'], 2),\n",
    "        'Final Fatigue': round(eval['final_fatigue'], 2),\n",
    "        'Final Risk': round(eval['final_risk'], 3),\n",
    "        'Goal Achieved': eval['goal_achieved'],\n",
    "        'Rest Days': eval['rest_days'],\n",
    "        'High Intensity Days': eval['high_intensity_days'],\n",
    "        'Total Workload': round(eval['total_workload'], 2)\n",
    "    }\n",
    "    for eval in evaluations\n",
    "])\n",
    "\n",
    "# Display the comparison\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbff367",
   "metadata": {},
   "source": [
    "## Time and Space Complexity Comparison\n",
    "\n",
    "Let's compare the computational efficiency of the three algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f02690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for algorithm performance metrics\n",
    "performance_df = pd.DataFrame([\n",
    "    {\n",
    "        'Algorithm': 'CSP',\n",
    "        'Execution Time (s)': csp_result['execution_time'],\n",
    "        'Peak Memory (MB)': csp_result['peak_memory'],\n",
    "        'Nodes Expanded': csp_stats['iterations'],\n",
    "        'Max Frontier Size': 'N/A (Recursive)'\n",
    "    },\n",
    "    {\n",
    "        'Algorithm': 'DFS',\n",
    "        'Execution Time (s)': dfs_result['execution_time'],\n",
    "        'Peak Memory (MB)': dfs_result['peak_memory'],\n",
    "        'Nodes Expanded': dfs_expanded_nodes,\n",
    "        'Max Frontier Size': dfs_max_stack_size\n",
    "    },\n",
    "    {\n",
    "        'Algorithm': 'BFS',\n",
    "        'Execution Time (s)': bfs_result['execution_time'],\n",
    "        'Peak Memory (MB)': bfs_result['peak_memory'],\n",
    "        'Nodes Expanded': bfs_expanded_nodes,\n",
    "        'Max Frontier Size': bfs_max_queue_size\n",
    "    },\n",
    "    {\n",
    "        'Algorithm': 'UCS',\n",
    "        'Execution Time (s)': ucs_result['execution_time'],\n",
    "        'Peak Memory (MB)': ucs_result['peak_memory'],\n",
    "        'Nodes Expanded': ucs_expanded_nodes,\n",
    "        'Max Frontier Size': ucs_max_queue_size\n",
    "    },\n",
    "    {\n",
    "        'Algorithm': 'Greedy',\n",
    "        'Execution Time (s)': greedy_result['execution_time'],\n",
    "        'Peak Memory (MB)': greedy_result['peak_memory'],\n",
    "        'Nodes Expanded': greedy_expanded_nodes,\n",
    "        'Max Frontier Size': greedy_max_queue_size\n",
    "    },\n",
    "    {\n",
    "        'Algorithm': 'A*',\n",
    "        'Execution Time (s)': astar_result['execution_time'],\n",
    "        'Peak Memory (MB)': astar_result['peak_memory'],\n",
    "        'Nodes Expanded': astar_expanded_nodes,\n",
    "        'Max Frontier Size': astar_max_queue_size\n",
    "    }\n",
    "    # {\n",
    "    #     'Algorithm': 'Genetic',\n",
    "    #     'Execution Time (s)': genetic_result['execution_time'],\n",
    "    #     'Peak Memory (MB)': genetic_result['peak_memory'],\n",
    "    #     'Nodes Expanded': 'N/A',\n",
    "    #     'Max Frontier Size': 'N/A'\n",
    "    # }\n",
    "])\n",
    "\n",
    "# Display the performance comparison\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604dd465",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize the comparison between algorithms using various plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d64484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot execution time comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Algorithm', y='Execution Time (s)', data=performance_df)\n",
    "plt.title('Execution Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot memory usage comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Algorithm', y='Peak Memory (MB)', data=performance_df)\n",
    "plt.title('Peak Memory Usage Comparison')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot nodes expanded comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "numeric_nodes = pd.DataFrame([\n",
    "    {'Algorithm': algo, 'Nodes Expanded': nodes if isinstance(nodes, (int, float)) else 0}\n",
    "    for algo, nodes in zip(performance_df['Algorithm'], performance_df['Nodes Expanded'])\n",
    "])\n",
    "sns.barplot(x='Algorithm', y='Nodes Expanded', data=numeric_nodes)\n",
    "plt.title('Nodes Expanded Comparison')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925c49a",
   "metadata": {},
   "source": [
    "## Solution Quality Visualization\n",
    "\n",
    "Let's visualize how the performance, fatigue, and risk metrics change over the training period for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab88e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance progression\n",
    "plt.figure(figsize=(12, 6))\n",
    "for eval in evaluations:\n",
    "    days = list(range(len(eval['performance_progression'])))\n",
    "    plt.plot(days, eval['performance_progression'], marker='o', label=eval['algorithm'])\n",
    "\n",
    "plt.axhline(y=TARGET_PERF, color='r', linestyle='--', label='Target Performance')\n",
    "plt.title('Performance Progression Comparison')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Performance Level')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot fatigue progression\n",
    "plt.figure(figsize=(12, 6))\n",
    "for eval in evaluations:\n",
    "    days = list(range(len(eval['fatigue_progression'])))\n",
    "    plt.plot(days, eval['fatigue_progression'], marker='o', label=eval['algorithm'])\n",
    "\n",
    "plt.axhline(y=MAX_FATIGUE, color='r', linestyle='--', label='Max Fatigue')\n",
    "plt.title('Fatigue Progression Comparison')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Fatigue Level')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot risk progression\n",
    "plt.figure(figsize=(12, 6))\n",
    "for eval in evaluations:\n",
    "    days = list(range(len(eval['risk_progression'])))\n",
    "    plt.plot(days, eval['risk_progression'], marker='o', label=eval['algorithm'])\n",
    "\n",
    "plt.axhline(y=MAX_RISK, color='r', linestyle='--', label='Max Risk')\n",
    "plt.title('Injury Risk Progression Comparison')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Injury Risk')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e4ab8",
   "metadata": {},
   "source": [
    "## Training Schedule Visualization\n",
    "\n",
    "Let's visualize the actual training schedules produced by each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_schedule(evaluation, title):\n",
    "    if evaluation is None or not evaluation['actions']:\n",
    "        print(f\"No schedule available for {title}\")\n",
    "        return\n",
    "    \n",
    "    actions = evaluation['actions']\n",
    "    intensities = [action[0] for action in actions]\n",
    "    durations = [action[1] for action in actions]\n",
    "    days = list(range(1, len(actions) + 1))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    \n",
    "    # Plot intensities\n",
    "    ax1.bar(days, intensities, color='skyblue')\n",
    "    ax1.set_ylabel('Training Intensity')\n",
    "    ax1.set_title(f'{title} - Training Schedule')\n",
    "    ax1.grid(True, axis='y')\n",
    "    \n",
    "    # Plot durations\n",
    "    ax2.bar(days, durations, color='salmon')\n",
    "    ax2.set_xlabel('Day')\n",
    "    ax2.set_ylabel('Duration (minutes)')\n",
    "    ax2.grid(True, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize each algorithm's schedule\n",
    "for eval in evaluations:\n",
    "    visualize_schedule(eval, eval['algorithm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdbc4b",
   "metadata": {},
   "source": [
    "## Combined Training Schedule Comparison\n",
    "\n",
    "Let's create a side-by-side comparison of all training schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap showing intensity across days for each algorithm\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "intensity_data = []\n",
    "algorithm_labels = []\n",
    "\n",
    "for eval in evaluations:\n",
    "    if eval and eval['actions']:\n",
    "        # Get intensities from actions\n",
    "        intensities = [action[0] for action in eval['actions']]\n",
    "        # Pad to ensure all are the same length\n",
    "        while len(intensities) < TARGET_DAY:\n",
    "            intensities.append(0)\n",
    "        intensity_data.append(intensities[:TARGET_DAY])\n",
    "        algorithm_labels.append(eval['algorithm'])\n",
    "\n",
    "if intensity_data:\n",
    "    # Create a heatmap\n",
    "    # Store the return value from sns.heatmap() and use the colorbar parameter\n",
    "    heatmap = sns.heatmap(intensity_data, cmap=\"YlOrRd\", \n",
    "                 xticklabels=list(range(1, TARGET_DAY+1)),\n",
    "                 yticklabels=algorithm_labels,\n",
    "                 cbar_kws={'label': 'Intensity'})\n",
    "    plt.title('Training Intensity by Day')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Algorithm')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c039a8b",
   "metadata": {},
   "source": [
    "## Schedule Property Comparison\n",
    "\n",
    "Let's compare the rest days, high intensity days, and total workload across algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rest days comparison\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x='Algorithm', y='Rest Days', data=comparison_df)\n",
    "plt.title('Rest Days')\n",
    "plt.ylim(0, TARGET_DAY)\n",
    "\n",
    "# Plot high intensity days comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x='Algorithm', y='High Intensity Days', data=comparison_df)\n",
    "plt.title('High Intensity Days')\n",
    "plt.ylim(0, TARGET_DAY)\n",
    "\n",
    "# Plot total workload comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x='Algorithm', y='Total Workload', data=comparison_df)\n",
    "plt.title('Total Workload')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9f8bb",
   "metadata": {},
   "source": [
    "## Radar Chart Comparison\n",
    "\n",
    "Let's use a radar chart to visualize multiple metrics at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_chart(data, categories, title):\n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Compute angle for each axis\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Initialize the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Draw the chart for each algorithm\n",
    "    for i, (algo, values) in enumerate(data.items()):\n",
    "        values_closed = values + values[:1]  # Close the loop\n",
    "        ax.plot(angles, values_closed, linewidth=2, linestyle='solid', label=algo)\n",
    "        ax.fill(angles, values_closed, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(title, size=15, y=1.1)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Prepare data for radar chart - normalize all metrics to a 0-1 scale\n",
    "radar_data = {}\n",
    "categories = ['Performance', 'Low Risk', 'Low Fatigue', 'Rest Balance', 'Workload', 'Execution Speed']\n",
    "\n",
    "for eval in evaluations:\n",
    "    algorithm = eval['algorithm']\n",
    "    \n",
    "    # Get algorithm execution time\n",
    "    if algorithm == 'CSP':\n",
    "        exec_time = csp_result['execution_time']\n",
    "    elif algorithm == 'DFS':\n",
    "        exec_time = dfs_result['execution_time']\n",
    "    elif algorithm == 'BFS':\n",
    "        exec_time = bfs_result['execution_time']\n",
    "    elif algorithm == 'UCS':\n",
    "        exec_time = ucs_result['execution_time']\n",
    "    elif algorithm == 'A*':\n",
    "        exec_time = astar_result['execution_time']\n",
    "    else:  # Greedy\n",
    "        exec_time = greedy_result['execution_time']\n",
    "    \n",
    "    # Normalize metrics\n",
    "    perf_norm = eval['final_performance'] / 10  # Performance on 0-10 scale\n",
    "    risk_norm = 1 - (eval['final_risk'] / 1)  # Invert risk (lower is better)\n",
    "    fatigue_norm = 1 - (eval['final_fatigue'] / 5)  # Invert fatigue (lower is better)\n",
    "    rest_norm = eval['rest_days'] / TARGET_DAY  # Rest day proportion\n",
    "    workload_norm = min(1.0, eval['total_workload'] / 1000)  # Cap at reasonable value\n",
    "    speed_norm = 1 - min(1.0, exec_time / max(1, csp_result['execution_time'], greedy_result['execution_time']))  # Invert time (lower is better)\n",
    "    \n",
    "    radar_data[algorithm] = [perf_norm, risk_norm, fatigue_norm, rest_norm, workload_norm, speed_norm]\n",
    "\n",
    "# Create the radar chart\n",
    "if radar_data:\n",
    "    radar_chart(radar_data, categories, 'Algorithm Comparison - Multiple Metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb473f",
   "metadata": {},
   "source": [
    "## Export Data for Chart.js\n",
    "\n",
    "Here we'll extract the data from our visualizations in a format compatible with Chart.js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7921b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract data for Chart.js\n",
    "def extract_for_chartjs(data_dict):\n",
    "    return json.dumps(data_dict, indent=2)\n",
    "\n",
    "# Create Chart.js data structure for performance metrics\n",
    "performance_chartjs = {\n",
    "    \"type\": \"bar\",\n",
    "    \"data\": {\n",
    "        \"labels\": performance_df[\"Algorithm\"].tolist(),\n",
    "        \"datasets\": [\n",
    "            {\n",
    "                \"label\": \"Execution Time (s)\",\n",
    "                \"data\": performance_df[\"Execution Time (s)\"].tolist(),\n",
    "                \"backgroundColor\": \"rgba(54, 162, 235, 0.5)\",\n",
    "                \"borderColor\": \"rgba(54, 162, 235, 1)\",\n",
    "                \"borderWidth\": 1\n",
    "            },\n",
    "            {\n",
    "                \"label\": \"Peak Memory (MB)\",\n",
    "                \"data\": performance_df[\"Peak Memory (MB)\"].tolist(),\n",
    "                \"backgroundColor\": \"rgba(255, 99, 132, 0.5)\",\n",
    "                \"borderColor\": \"rgba(255, 99, 132, 1)\",\n",
    "                \"borderWidth\": 1\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"options\": {\n",
    "        \"responsive\": True,\n",
    "        \"scales\": {\n",
    "            \"y\": {\n",
    "                \"beginAtZero\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create Chart.js data for solution quality comparison\n",
    "solution_quality_chartjs = {\n",
    "    \"type\": \"bar\",\n",
    "    \"data\": {\n",
    "        \"labels\": comparison_df[\"Algorithm\"].tolist(),\n",
    "        \"datasets\": [\n",
    "            {\n",
    "                \"label\": \"Final Performance\",\n",
    "                \"data\": comparison_df[\"Final Performance\"].tolist(),\n",
    "                \"backgroundColor\": \"rgba(75, 192, 192, 0.5)\",\n",
    "                \"borderColor\": \"rgba(75, 192, 192, 1)\",\n",
    "                \"borderWidth\": 1\n",
    "            },\n",
    "            {\n",
    "                \"label\": \"Final Fatigue\",\n",
    "                \"data\": comparison_df[\"Final Fatigue\"].tolist(),\n",
    "                \"backgroundColor\": \"rgba(255, 206, 86, 0.5)\",\n",
    "                \"borderColor\": \"rgba(255, 206, 86, 1)\",\n",
    "                \"borderWidth\": 1\n",
    "            },\n",
    "            {\n",
    "                \"label\": \"Final Risk\",\n",
    "                \"data\": comparison_df[\"Final Risk\"].tolist(),\n",
    "                \"backgroundColor\": \"rgba(153, 102, 255, 0.5)\",\n",
    "                \"borderColor\": \"rgba(153, 102, 255, 1)\",\n",
    "                \"borderWidth\": 1\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Extract progression data (performance, fatigue, risk)\n",
    "progression_chartjs = {\n",
    "    \"performance\": {},\n",
    "    \"fatigue\": {},\n",
    "    \"risk\": {}\n",
    "}\n",
    "\n",
    "# Set color palette for consistency\n",
    "colors = [\n",
    "    \"rgba(255, 99, 132, 1)\",  # Red\n",
    "    \"rgba(54, 162, 235, 1)\",  # Blue\n",
    "    \"rgba(255, 206, 86, 1)\",  # Yellow\n",
    "    \"rgba(75, 192, 192, 1)\",  # Green\n",
    "    \"rgba(153, 102, 255, 1)\",  # Purple\n",
    "    \"rgba(255, 159, 64, 1)\",  # Orange\n",
    "    \"rgba(201, 203, 207, 1)\"  # Grey\n",
    "]\n",
    "\n",
    "# Create performance progression data\n",
    "datasets = []\n",
    "for i, eval in enumerate(evaluations):\n",
    "    color_idx = i % len(colors)\n",
    "    datasets.append({\n",
    "        \"label\": eval[\"algorithm\"],\n",
    "        \"data\": eval[\"performance_progression\"],\n",
    "        \"borderColor\": colors[color_idx],\n",
    "        \"backgroundColor\": colors[color_idx].replace(\"1)\", \"0.2)\"),\n",
    "        \"fill\": False,\n",
    "        \"tension\": 0.1\n",
    "    })\n",
    "\n",
    "progression_chartjs[\"performance\"] = {\n",
    "    \"type\": \"line\",\n",
    "    \"data\": {\n",
    "        \"labels\": list(range(len(evaluations[0][\"performance_progression\"]))),\n",
    "        \"datasets\": datasets\n",
    "    },\n",
    "    \"options\": {\n",
    "        \"responsive\": True,\n",
    "        \"plugins\": {\n",
    "            \"title\": {\n",
    "                \"display\": True,\n",
    "                \"text\": \"Performance Progression\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create fatigue progression data\n",
    "datasets = []\n",
    "for i, eval in enumerate(evaluations):\n",
    "    color_idx = i % len(colors)\n",
    "    datasets.append({\n",
    "        \"label\": eval[\"algorithm\"],\n",
    "        \"data\": eval[\"fatigue_progression\"],\n",
    "        \"borderColor\": colors[color_idx],\n",
    "        \"backgroundColor\": colors[color_idx].replace(\"1)\", \"0.2)\"),\n",
    "        \"fill\": False,\n",
    "        \"tension\": 0.1\n",
    "    })\n",
    "\n",
    "progression_chartjs[\"fatigue\"] = {\n",
    "    \"type\": \"line\",\n",
    "    \"data\": {\n",
    "        \"labels\": list(range(len(evaluations[0][\"fatigue_progression\"]))),\n",
    "        \"datasets\": datasets\n",
    "    },\n",
    "    \"options\": {\n",
    "        \"responsive\": True,\n",
    "        \"plugins\": {\n",
    "            \"title\": {\n",
    "                \"display\": True,\n",
    "                \"text\": \"Fatigue Progression\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create risk progression data\n",
    "datasets = []\n",
    "for i, eval in enumerate(evaluations):\n",
    "    color_idx = i % len(colors)\n",
    "    datasets.append({\n",
    "        \"label\": eval[\"algorithm\"],\n",
    "        \"data\": eval[\"risk_progression\"],\n",
    "        \"borderColor\": colors[color_idx],\n",
    "        \"backgroundColor\": colors[color_idx].replace(\"1)\", \"0.2)\"),\n",
    "        \"fill\": False,\n",
    "        \"tension\": 0.1\n",
    "    })\n",
    "\n",
    "progression_chartjs[\"risk\"] = {\n",
    "    \"type\": \"line\",\n",
    "    \"data\": {\n",
    "        \"labels\": list(range(len(evaluations[0][\"risk_progression\"]))),\n",
    "        \"datasets\": datasets\n",
    "    },\n",
    "    \"options\": {\n",
    "        \"responsive\": True,\n",
    "        \"plugins\": {\n",
    "            \"title\": {\n",
    "                \"display\": True,\n",
    "                \"text\": \"Risk Progression\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Extract training schedule data for each algorithm\n",
    "schedule_chartjs = {}\n",
    "for eval in evaluations:\n",
    "    if eval and eval[\"actions\"]:\n",
    "        algo = eval[\"algorithm\"]\n",
    "        intensities = [action[0] for action in eval[\"actions\"]]\n",
    "        durations = [action[1] for action in eval[\"actions\"]]\n",
    "        days = list(range(1, len(eval[\"actions\"]) + 1))\n",
    "        \n",
    "        # Create separate datasets for intensity and duration\n",
    "        schedule_chartjs[algo] = {\n",
    "            \"type\": \"bar\",\n",
    "            \"data\": {\n",
    "                \"labels\": days,\n",
    "                \"datasets\": [\n",
    "                    {\n",
    "                        \"label\": \"Intensity\",\n",
    "                        \"data\": intensities,\n",
    "                        \"backgroundColor\": \"rgba(54, 162, 235, 0.5)\",\n",
    "                        \"borderColor\": \"rgba(54, 162, 235, 1)\",\n",
    "                        \"yAxisID\": \"y\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"Duration\",\n",
    "                        \"data\": durations,\n",
    "                        \"backgroundColor\": \"rgba(255, 99, 132, 0.5)\",\n",
    "                        \"borderColor\": \"rgba(255, 99, 132, 1)\",\n",
    "                        \"yAxisID\": \"y1\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"options\": {\n",
    "                \"responsive\": True,\n",
    "                \"scales\": {\n",
    "                    \"y\": {\n",
    "                        \"type\": \"linear\",\n",
    "                        \"display\": True,\n",
    "                        \"position\": \"left\",\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Intensity\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"y1\": {\n",
    "                        \"type\": \"linear\",\n",
    "                        \"display\": True,\n",
    "                        \"position\": \"right\",\n",
    "                        \"title\": {\n",
    "                            \"display\": True,\n",
    "                            \"text\": \"Duration (minutes)\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"plugins\": {\n",
    "                    \"title\": {\n",
    "                        \"display\": True,\n",
    "                        \"text\": f\"{algo} Training Schedule\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create radar chart data\n",
    "radar_chartjs = {\n",
    "    \"type\": \"radar\",\n",
    "    \"data\": {\n",
    "        \"labels\": categories,\n",
    "        \"datasets\": []\n",
    "    },\n",
    "    \"options\": {\n",
    "        \"elements\": {\n",
    "            \"line\": {\n",
    "                \"borderWidth\": 3\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add datasets for each algorithm\n",
    "for i, (algo, values) in enumerate(radar_data.items()):\n",
    "    color_idx = i % len(colors)\n",
    "    radar_chartjs[\"data\"][\"datasets\"].append({\n",
    "        \"label\": algo,\n",
    "        \"data\": values,\n",
    "        \"fill\": True,\n",
    "        \"backgroundColor\": colors[color_idx].replace(\"1)\", \"0.2)\"),\n",
    "        \"borderColor\": colors[color_idx],\n",
    "        \"pointBackgroundColor\": colors[color_idx],\n",
    "        \"pointBorderColor\": \"#fff\",\n",
    "        \"pointHoverBackgroundColor\": \"#fff\",\n",
    "        \"pointHoverBorderColor\": colors[color_idx]\n",
    "    })\n",
    "\n",
    "# Create heatmap data (training intensity comparison)\n",
    "heatmap_data = {\n",
    "    \"algorithms\": algorithm_labels,\n",
    "    \"days\": list(range(1, TARGET_DAY + 1)),\n",
    "    \"intensities\": []\n",
    "}\n",
    "\n",
    "for data_row in intensity_data:\n",
    "    heatmap_data[\"intensities\"].append(data_row)\n",
    "\n",
    "# Create property comparison chart (rest days, high intensity days, workload)\n",
    "property_chartjs = {\n",
    "    \"type\": \"bar\",\n",
    "    \"data\": {\n",
    "        \"labels\": comparison_df[\"Algorithm\"].tolist(),\n",
    "        \"datasets\": [\n",
    "            {\n",
    "                \"label\": \"Rest Days\",\n",
    "                \"data\": comparison_df[\"Rest Days\"].tolist(),\n",
    "                \"backgroundColor\": \"rgba(75, 192, 192, 0.5)\"\n",
    "            },\n",
    "            {\n",
    "                \"label\": \"High Intensity Days\",\n",
    "                \"data\": comparison_df[\"High Intensity Days\"].tolist(),\n",
    "                \"backgroundColor\": \"rgba(255, 99, 132, 0.5)\"\n",
    "            },\n",
    "            {\n",
    "                \"label\": \"Total Workload (scaled)\",\n",
    "                \"data\": comparison_df.apply(lambda x: x[\"Total Workload\"] / 100, axis=1).tolist(),  # Scale down for better visualization\n",
    "                \"backgroundColor\": \"rgba(54, 162, 235, 0.5)\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Compile all data into one JSON object\n",
    "chartjs_data = {\n",
    "    \"performance\": performance_chartjs,\n",
    "    \"solutionQuality\": solution_quality_chartjs,\n",
    "    \"progressions\": progression_chartjs,\n",
    "    \"schedules\": schedule_chartjs,\n",
    "    \"radar\": radar_chartjs,\n",
    "    \"heatmap\": heatmap_data,\n",
    "    \"properties\": property_chartjs,\n",
    "    \"rawData\": {\n",
    "        \"performanceComparison\": performance_df.to_dict(\"records\"),\n",
    "        \"solutionComparison\": comparison_df.to_dict(\"records\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Export to JSON file\n",
    "with open('chartjs_data.json', 'w') as f:\n",
    "    json.dump(chartjs_data, f, indent=2)\n",
    "    \n",
    "print(\"Chart.js data has been exported to 'chartjs_data.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
